{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pathgan-torch",
   "display_name": "Python 3.8 - pathgan-torch",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Kernel padding experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_channels = 1\n",
    "in_channels = 1\n",
    "kernel_size = (2,2)\n",
    "weights = torch.ones(out_channels,in_channels, kernel_size[0], kernel_size[1])\n",
    "print(weights.shape)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = F.pad(weights, [1, 1, 1, 1])\n",
    "print(padded.shape)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_upscale = padded[:, :, 1:, 1:] + padded[:, :, 1:, :-1] + padded[:, :, :-1, 1:] + padded[:, :, :-1, :-1]\n",
    "\n",
    "print(padded[:, :, 1:, 1:], '\\n\\n', padded[:, :, 1:, :-1], '\\n\\n', padded[:, :, :-1, 1:], '\\n\\n', padded[:, :, :-1, :-1])\n",
    "\n",
    "print(padded_upscale.shape)\n",
    "print(padded_upscale /4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mat = weights.view(weights.size(0), -1)\n",
    "w_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import max_singular_value\n",
    "u = torch.FloatTensor(1, w_mat.size(0)).normal_(0, 1).cpu()\n",
    "sigma, _u = max_singular_value(w_mat, u, 1)\n",
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_upscale/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weights[:, :, 1:, 1:] + weights[:, :, 1:, :-\n",
    "                                            1] + weights[:, :, :-1, 1:] + weights[:, :, :-1, :-1]\n",
    "w_mat = weights.view(weights.size(0), -1)\n",
    "sigma, _u = max_singular_value(w_mat, self.u, 1)\n",
    "self.u.copy_(_u)\n",
    "return weights / sigma"
   ]
  },
  {
   "source": [
    "# _output_padding experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def _output_padding(input, output_padding, output_size, stride, padding, kernel_size, dilation=None):\n",
    "        # type: (Tensor, Optional[List[int]], List[int], List[int], List[int], Optional[List[int]]) -> List[int]\n",
    "        if output_size is None:\n",
    "            ret = tuple(output_padding)  # converting to list if was not already\n",
    "        else:\n",
    "            k = input.dim() - 2\n",
    "            if len(output_size) == k + 2:\n",
    "                output_size = output_size[2:]\n",
    "            if len(output_size) != k:\n",
    "                raise ValueError(\n",
    "                    \"output_size must have {} or {} elements (got {})\"\n",
    "                    .format(k, k + 2, len(output_size)))\n",
    "\n",
    "            min_sizes = torch.jit.annotate(List[int], [])\n",
    "            max_sizes = torch.jit.annotate(List[int], [])\n",
    "            for d in range(k):\n",
    "                dim_size = ((input.size(d + 2) - 1) * stride[d] -\n",
    "                            2 * padding[d] +\n",
    "                            (dilation[d] if dilation is not None else 1) * (kernel_size[d] - 1) + 1)\n",
    "                min_sizes.append(dim_size)\n",
    "                max_sizes.append(min_sizes[d] + stride[d] - 1)\n",
    "\n",
    "            for i in range(len(output_size)):\n",
    "                size = output_size[i]\n",
    "                min_size = min_sizes[i]\n",
    "                max_size = max_sizes[i]\n",
    "                if size < min_size or size > max_size:\n",
    "                    raise ValueError((\n",
    "                        \"requested an output size of {}, but valid sizes range \"\n",
    "                        \"from {} to {} (for an input of {})\").format(\n",
    "                            output_size, min_sizes, max_sizes, input.size()[2:]))\n",
    "\n",
    "            res = torch.jit.annotate(List[int], [])\n",
    "            for d in range(k):\n",
    "                res.append(output_size[d] - min_sizes[d])\n",
    "\n",
    "            ret = res\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = torch.rand((64, 3, 224, 224))\n",
    "\n",
    "output_padding = _output_padding(inputs, output_padding=[0,0], output_size=[input_x * 2,input_y * 2], stride=[2,2], padding=[1,1], kernel_size=[3,3], dilation=None)\n",
    "\n",
    "output_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_maintain_size(dilation, kernel_size):\n",
    "    effective_filter_size = dilation * (kernel_size - 1) + 1\n",
    "    if effective_filter_size % 2 == 0:\n",
    "        print(\"In order to maintain input image size, effective filter size must be odd\")\n",
    "    return (effective_filter_size - 1) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for stride in range(1,4):\n",
    "        for dilation in range(1,3):\n",
    "            for kernel_size in range(1,6, 2):\n",
    "                failure = False\n",
    "                sizes = []\n",
    "\n",
    "                padding = pad_to_maintain_size(dilation, kernel_size)\n",
    "\n",
    "                for input_size in range(0, 100):\n",
    "                    output_size = input_size * 2\n",
    "                    min_size = (input_size - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + 1\n",
    "                    max_size = min_size + stride - 1\n",
    "                    if output_size < min_size or output_size > max_size:\n",
    "                        failure = True\n",
    "                        sizes.append(input_x)\n",
    "\n",
    "                message = 'fail' if failure else 'success'\n",
    "                print(f'{message} stride: {stride}, padding: {padding}, dilation: {dilation}, kernel_size: {kernel_size}, failed sizes: {len(sizes)}')\n",
    "        \n",
    "       "
   ]
  },
  {
   "source": [
    "from https://stats.stackexchange.com/questions/297678/how-to-calculate-optimal-zero-padding-for-convolutional-neural-networks\n",
    "\n",
    "The possible values for the padding size, ð‘ƒ, depends the input size ð‘Š (following the notation of the blog), the filter size ð¹ and the stride ð‘†. We assume width and height are the same.\n",
    "\n",
    "What you need to ensure is that the output size, (ð‘Šâˆ’ð¹+2ð‘ƒ)/ð‘†+1, is an integer. When ð‘†=1 then you get your first equation ð‘ƒ=(ð¹âˆ’1)/2 as necessary condition. But, in general, you need to consider the three parameters, namely ð‘Š, ð¹ and ð‘† in order to determine valid values of ð‘ƒ.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Same padding experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import Union\n",
    "\n",
    "def same_padding(conv_layer: Union[nn.Conv2d, nn.ConvTranspose2d], inputs: Tensor):\n",
    "    in_height = inputs.size(2)\n",
    "    in_width = inputs.size(3)\n",
    "    dilation = conv_layer.dilation\n",
    "    kernel_size = conv_layer.kernel_size\n",
    "    filter_size = (dilation[0] * (kernel_size[0] - 1) + 1, dilation[1] * (kernel_size[1] - 1) + 1)\n",
    "\n",
    "    print((in_height, in_width))\n",
    "    print(filter_size)\n",
    "\n",
    "    pad_along_height = dilation[0] * (kernel_size[0] - 1)\n",
    "    pad_along_width = dilation[1] * (kernel_size[1] - 1)\n",
    "\n",
    "    pad_top = pad_along_height // 2\n",
    "    pad_bottom = pad_along_height - pad_top\n",
    "    pad_left = pad_along_width // 2\n",
    "    pad_right = pad_along_width - pad_left\n",
    "\n",
    "    print(f'height: {pad_along_height}, width: {pad_along_width}')\n",
    "    print(f'bottom: {pad_bottom}, top: {pad_top}, left: {pad_left}, right: {pad_right}')\n",
    "\n",
    "    return F.pad(inputs, (pad_left, pad_right, pad_top, pad_bottom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand((64, 3, 122, 51))\n",
    "\n",
    "conv_layer = nn.Conv2d(in_channels=3,out_channels=3,kernel_size=(3,5), stride=1, dilation=1)\n",
    "\n",
    "padded_inputs = same_padding(conv_layer, inputs)\n",
    "output = conv_layer(padded_inputs)\n",
    "\n",
    "print(inputs.size())\n",
    "print(padded_inputs.size())\n",
    "print(output.size())"
   ]
  },
  {
   "source": [
    "# AdaIN experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple\n",
    "import unittest\n",
    "import torch\n",
    "from torch.tensor import Tensor\n",
    "import torch.nn as nn\n",
    "from models.generative.normalization.AdaptiveInstanceNormalization import AdaptiveInstanceNormalization\n",
    "\n",
    "class TestAdaIN():\n",
    "    #unit tests from https://zhangruochi.com/Components-of-StyleGAN/2020/10/13/\n",
    "    def setUp(self) -> None:\n",
    "        self.w_channels = 3\n",
    "        self.intermediate_channels = None\n",
    "        self.image_channels = 2\n",
    "        image_size = 3\n",
    "        n_test = 1\n",
    "        self.input_shape = (n_test, self.image_channels, image_size, image_size)\n",
    "        self.test_input = torch.ones(self.input_shape)\n",
    "        self.test_w = torch.ones(n_test, self.w_channels)\n",
    "        self.adain = AdaptiveInstanceNormalization(self.image_channels, self.w_channels, self.intermediate_channels)\n",
    "\n",
    "        nn.init.constant_(self.adain.gamma_layer.weight, 0.25)\n",
    "        nn.init.constant_(self.adain.beta_layer.weight, 0.2)\n",
    "        \n",
    "        for bias in (self.adain.gamma_layer.bias, self.adain.beta_layer.bias):\n",
    "            nn.init.zeros_(bias)\n",
    "            \n",
    "        if self.adain.dense_layer is not None:\n",
    "            nn.init.constant_(self.adain.dense_layer.weight, 0.2)\n",
    "            nn.init.zeros_(self.adain.dense_layer.bias)\n",
    "\n",
    "        self.test_w = torch.ones(n_test, self.w_channels)\n",
    "        \n",
    "    def test_layer_shape_correct(self):\n",
    "        assert self.adain.gamma_layer(self.test_w).shape == self.adain.beta_layer(self.test_w).shape\n",
    "        assert self.adain.gamma_layer(self.test_w).shape[-1] == self.image_channels\n",
    "        assert tuple(self.adain(self.test_input, self.test_w).shape) == self.input_shape\n",
    "    \n",
    "    def test_conditional_norm(self) -> None:\n",
    "        self.test_input[:, :, 0] = 0\n",
    "        test_output = self.adain(self.test_input, self.test_w)\n",
    "        assert(torch.abs(test_output[0, 0, 0, 0] - 3 / 5 + torch.sqrt(torch.tensor(9 / 8))) < 1e-4)\n",
    "        assert(torch.abs(test_output[0, 0, 1, 0] - 3 / 5 - torch.sqrt(torch.tensor(9 / 32))) < 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adain = TestAdaIN()\n",
    "test_adain.setUp()\n",
    "test_input = test_adain.test_input\n",
    "test_input[:,:,0] = 0\n",
    "test_w = test_adain.test_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = test_adain.adain(test_input, test_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}