{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pathgan-torch",
   "display_name": "Python 3.8 - pathgan-torch",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Kernel padding experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_channels = 1\n",
    "in_channels = 1\n",
    "kernel_size = (2,2)\n",
    "weights = torch.ones(out_channels,in_channels, kernel_size[0], kernel_size[1])\n",
    "print(weights.shape)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = F.pad(weights, [1, 1, 1, 1])\n",
    "print(padded.shape)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_upscale = padded[:, :, 1:, 1:] + padded[:, :, 1:, :-1] + padded[:, :, :-1, 1:] + padded[:, :, :-1, :-1]\n",
    "\n",
    "print(padded[:, :, 1:, 1:], '\\n\\n', padded[:, :, 1:, :-1], '\\n\\n', padded[:, :, :-1, 1:], '\\n\\n', padded[:, :, :-1, :-1])\n",
    "\n",
    "print(padded_upscale.shape)\n",
    "print(padded_upscale /4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mat = weights.view(weights.size(0), -1)\n",
    "w_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import max_singular_value\n",
    "u = torch.FloatTensor(1, w_mat.size(0)).normal_(0, 1).cpu()\n",
    "sigma, _u = max_singular_value(w_mat, u, 1)\n",
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_upscale/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weights[:, :, 1:, 1:] + weights[:, :, 1:, :-\n",
    "                                            1] + weights[:, :, :-1, 1:] + weights[:, :, :-1, :-1]\n",
    "w_mat = weights.view(weights.size(0), -1)\n",
    "sigma, _u = max_singular_value(w_mat, self.u, 1)\n",
    "self.u.copy_(_u)\n",
    "return weights / sigma"
   ]
  },
  {
   "source": [
    "# _output_padding experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def _output_padding(input, output_padding, output_size, stride, padding, kernel_size, dilation=None):\n",
    "        # type: (Tensor, Optional[List[int]], List[int], List[int], List[int], Optional[List[int]]) -> List[int]\n",
    "        if output_size is None:\n",
    "            ret = tuple(output_padding)  # converting to list if was not already\n",
    "        else:\n",
    "            k = input.dim() - 2\n",
    "            if len(output_size) == k + 2:\n",
    "                output_size = output_size[2:]\n",
    "            if len(output_size) != k:\n",
    "                raise ValueError(\n",
    "                    \"output_size must have {} or {} elements (got {})\"\n",
    "                    .format(k, k + 2, len(output_size)))\n",
    "\n",
    "            min_sizes = torch.jit.annotate(List[int], [])\n",
    "            max_sizes = torch.jit.annotate(List[int], [])\n",
    "            for d in range(k):\n",
    "                dim_size = ((input.size(d + 2) - 1) * stride[d] -\n",
    "                            2 * padding[d] +\n",
    "                            (dilation[d] if dilation is not None else 1) * (kernel_size[d] - 1) + 1)\n",
    "                min_sizes.append(dim_size)\n",
    "                max_sizes.append(min_sizes[d] + stride[d] - 1)\n",
    "\n",
    "            for i in range(len(output_size)):\n",
    "                size = output_size[i]\n",
    "                min_size = min_sizes[i]\n",
    "                max_size = max_sizes[i]\n",
    "                if size < min_size or size > max_size:\n",
    "                    raise ValueError((\n",
    "                        \"requested an output size of {}, but valid sizes range \"\n",
    "                        \"from {} to {} (for an input of {})\").format(\n",
    "                            output_size, min_sizes, max_sizes, input.size()[2:]))\n",
    "\n",
    "            res = torch.jit.annotate(List[int], [])\n",
    "            for d in range(k):\n",
    "                res.append(output_size[d] - min_sizes[d])\n",
    "\n",
    "            ret = res\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = torch.rand((64, 3, 224, 224))\n",
    "\n",
    "output_padding = _output_padding(inputs, output_padding=[0,0], output_size=[input_x * 2,input_y * 2], stride=[2,2], padding=[1,1], kernel_size=[3,3], dilation=None)\n",
    "\n",
    "output_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_maintain_size(dilation, kernel_size):\n",
    "    effective_filter_size = dilation * (kernel_size - 1) + 1\n",
    "    if effective_filter_size % 2 == 0:\n",
    "        print(\"In order to maintain input image size, effective filter size must be odd\")\n",
    "    return (effective_filter_size - 1) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for stride in range(1,4):\n",
    "        for dilation in range(1,3):\n",
    "            for kernel_size in range(1,6, 2):\n",
    "                failure = False\n",
    "                sizes = []\n",
    "\n",
    "                padding = pad_to_maintain_size(dilation, kernel_size)\n",
    "\n",
    "                for input_size in range(0, 100):\n",
    "                    output_size = input_size * 2\n",
    "                    min_size = (input_size - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + 1\n",
    "                    max_size = min_size + stride - 1\n",
    "                    if output_size < min_size or output_size > max_size:\n",
    "                        failure = True\n",
    "                        sizes.append(input_x)\n",
    "\n",
    "                message = 'fail' if failure else 'success'\n",
    "                print(f'{message} stride: {stride}, padding: {padding}, dilation: {dilation}, kernel_size: {kernel_size}, failed sizes: {len(sizes)}')\n",
    "        \n",
    "       "
   ]
  },
  {
   "source": [
    "from https://stats.stackexchange.com/questions/297678/how-to-calculate-optimal-zero-padding-for-convolutional-neural-networks\n",
    "\n",
    "The possible values for the padding size, 𝑃, depends the input size 𝑊 (following the notation of the blog), the filter size 𝐹 and the stride 𝑆. We assume width and height are the same.\n",
    "\n",
    "What you need to ensure is that the output size, (𝑊−𝐹+2𝑃)/𝑆+1, is an integer. When 𝑆=1 then you get your first equation 𝑃=(𝐹−1)/2 as necessary condition. But, in general, you need to consider the three parameters, namely 𝑊, 𝐹 and 𝑆 in order to determine valid values of 𝑃.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Same padding experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import Union\n",
    "\n",
    "def same_padding(conv_layer: Union[nn.Conv2d, nn.ConvTranspose2d], inputs: Tensor):\n",
    "    in_height = inputs.size(2)\n",
    "    in_width = inputs.size(3)\n",
    "    dilation = conv_layer.dilation\n",
    "    kernel_size = conv_layer.kernel_size\n",
    "    filter_size = (dilation[0] * (kernel_size[0] - 1) + 1, dilation[1] * (kernel_size[1] - 1) + 1)\n",
    "\n",
    "    print((in_height, in_width))\n",
    "    print(filter_size)\n",
    "\n",
    "    pad_along_height = dilation[0] * (kernel_size[0] - 1)\n",
    "    pad_along_width = dilation[1] * (kernel_size[1] - 1)\n",
    "\n",
    "    pad_top = pad_along_height // 2\n",
    "    pad_bottom = pad_along_height - pad_top\n",
    "    pad_left = pad_along_width // 2\n",
    "    pad_right = pad_along_width - pad_left\n",
    "\n",
    "    print(f'height: {pad_along_height}, width: {pad_along_width}')\n",
    "    print(f'bottom: {pad_bottom}, top: {pad_top}, left: {pad_left}, right: {pad_right}')\n",
    "\n",
    "    return F.pad(inputs, (pad_left, pad_right, pad_top, pad_bottom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand((64, 3, 122, 51))\n",
    "\n",
    "conv_layer = nn.Conv2d(in_channels=3,out_channels=3,kernel_size=(3,5), stride=1, dilation=1)\n",
    "\n",
    "padded_inputs = same_padding(conv_layer, inputs)\n",
    "output = conv_layer(padded_inputs)\n",
    "\n",
    "print(inputs.size())\n",
    "print(padded_inputs.size())\n",
    "print(output.size())"
   ]
  },
  {
   "source": [
    "# AdaIN experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple\n",
    "import unittest\n",
    "import torch\n",
    "from torch.tensor import Tensor\n",
    "import torch.nn as nn\n",
    "from modules.normalization.AdaptiveInstanceNormalization import AdaptiveInstanceNormalization\n",
    "\n",
    "class TestAdaIN():\n",
    "    #unit tests from https://zhangruochi.com/Components-of-StyleGAN/2020/10/13/\n",
    "    def setUp(self) -> None:\n",
    "        self.w_channels = 3\n",
    "        self.intermediate_channels = None\n",
    "        self.image_channels = 2\n",
    "        image_size = 3\n",
    "        n_test = 1\n",
    "        self.input_shape = (n_test, self.image_channels, image_size, image_size)\n",
    "        self.test_input = torch.ones(self.input_shape)\n",
    "        self.test_w = torch.ones(n_test, self.w_channels)\n",
    "        self.adain = AdaptiveInstanceNormalization(self.image_channels, self.w_channels, self.intermediate_channels)\n",
    "\n",
    "        nn.init.constant_(self.adain.gamma_layer.weight, 0.25)\n",
    "        nn.init.constant_(self.adain.beta_layer.weight, 0.2)\n",
    "        \n",
    "        for bias in (self.adain.gamma_layer.bias, self.adain.beta_layer.bias):\n",
    "            nn.init.zeros_(bias)\n",
    "            \n",
    "        if self.adain.dense_layer is not None:\n",
    "            nn.init.constant_(self.adain.dense_layer.weight, 0.2)\n",
    "            nn.init.zeros_(self.adain.dense_layer.bias)\n",
    "\n",
    "        self.test_w = torch.ones(n_test, self.w_channels)\n",
    "        \n",
    "    def test_layer_shape_correct(self):\n",
    "        assert self.adain.gamma_layer(self.test_w).shape == self.adain.beta_layer(self.test_w).shape\n",
    "        assert self.adain.gamma_layer(self.test_w).shape[-1] == self.image_channels\n",
    "        assert tuple(self.adain(self.test_input, self.test_w).shape) == self.input_shape\n",
    "    \n",
    "    def test_conditional_norm(self) -> None:\n",
    "        self.test_input[:, :, 0] = 0\n",
    "        test_output = self.adain(self.test_input, self.test_w)\n",
    "        assert(torch.abs(test_output[0, 0, 0, 0] - 3 / 5 + torch.sqrt(torch.tensor(9 / 8))) < 1e-4)\n",
    "        assert(torch.abs(test_output[0, 0, 1, 0] - 3 / 5 - torch.sqrt(torch.tensor(9 / 32))) < 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "test_adain = TestAdaIN()\n",
    "test_adain.setUp()\n",
    "test_input = test_adain.test_input\n",
    "test_input[:,:,0] = 0\n",
    "test_w = test_adain.test_w\n",
    "test_input.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = test_adain.adain(test_input, test_w)"
   ]
  },
  {
   "source": [
    "# (more) Spectral Norm experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import unittest\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.utils import spectral_norm \n",
    "from torch.nn.utils.spectral_norm import SpectralNorm\n",
    "from torch.nn.init import constant_\n",
    "\n",
    "from models.generative.ConvolutionalBlock import ConvolutionalBlock\n",
    "from models.generative.ConvolutionalScale import DownscaleConv2d, FusedScale, UpscaleConv2d\n",
    "\n",
    "parameter_name = 'weight'\n",
    "\n",
    "input_shape = (1, 3, 6, 6)\n",
    "in_channels, out_channels, kernel_size = 3, 6, 3\n",
    "fn = lambda x: spectral_norm(x, name = parameter_name, dim = 0)\n",
    "scale = fn(DownscaleConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, fused_scale=True))\n",
    "nn.init.constant_(scale.weight, 1)\n",
    "dummy1 = copy.deepcopy(scale)\n",
    "dummy2 = copy.deepcopy(scale)\n",
    "\n",
    "data = torch.ones(input_shape)\n",
    "\n",
    "FusedScale(name=parameter_name)(dummy1, None)\n",
    "SpectralNorm(name=parameter_name, dim=0)(dummy1, None)\n",
    "dummy1_out = F.conv2d(data, dummy1.filter, dummy1.bias, stride=2, padding=dummy1.padding)\n",
    "\n",
    "SpectralNorm(name=parameter_name, dim=0)(dummy2, None)\n",
    "FusedScale(name=parameter_name)(dummy2, None)\n",
    "dummy2_out = F.conv2d(data, dummy2.filter, dummy2.bias, stride=2, padding=dummy2.padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.equal(dummy1_out, dummy2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.spectral_norm import SpectralNorm, spectral_norm\n",
    "import copy\n",
    "\n",
    "in_channels, out_channels, kernel_size = 1, 1, 3\n",
    "test_module = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "nn.init.constant_(test_module.weight, 1)\n",
    "\n",
    "test_module2 = copy.deepcopy(test_module)\n",
    "\n",
    "spectral_test1 = spectral_norm(test_module)\n",
    "spectral_test2 = spectral_norm(test_module2)\n",
    "\n",
    "print(spectral_test1.weight)\n",
    "print(spectral_test2.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "def fused_scale(weight: Tensor) -> Tensor:\n",
    "        padded = F.pad(weight, [1, 1, 1, 1])\n",
    "        filter = padded[:, :, 1:, 1:] + padded[:, :, 1:, :-1] + padded[:, :, :-1, 1:] + padded[:, :, :-1, :-1]\n",
    "        return filter\n",
    "\n",
    "def _l2normalize(v, eps=1e-12):\n",
    "    return v / (torch.norm(v) + eps)\n",
    "\n",
    "def max_singular_value(W, u=None, Ip=1):\n",
    "    \"\"\"\n",
    "    power iteration for weight parameter\n",
    "    \"\"\"\n",
    "    #xp = W.data\n",
    "    if Ip < 1:\n",
    "        raise ValueError(\"Power iteration should be a positive integer\")\n",
    "    if u is None:\n",
    "        u = torch.FloatTensor(1, W.size(0)).normal_(0, 1).cpu()\n",
    "    _u = u\n",
    "    for _ in range(Ip):\n",
    "        _v = _l2normalize(torch.matmul(_u, W.data), eps=1e-12)\n",
    "        _u = _l2normalize(torch.matmul(_v, torch.transpose(W.data, 0, 1)), eps=1e-12)\n",
    "    sigma = torch.sum(F.linear(_u, torch.transpose(W.data, 0, 1)) * _v) # type: ignore\n",
    "    return sigma, _u\n",
    "\n",
    "def W_(weights, u):\n",
    "    w_mat = weights.view(weights.size(0), -1)\n",
    "    sigma, _u = max_singular_value(w_mat, u)\n",
    "    u.copy_(_u) # type: ignore\n",
    "    return weights / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensor = torch.rand(1,1,3,3)\n",
    "u = torch.Tensor(1, 1).normal_()\n",
    "\n",
    "fused_scale_then_spectral = W_(fused_scale(tensor), u)\n",
    "spectral_then_fused_scale = fused_scale(W_(tensor,u))\n",
    "spectral_then_fused_then_spectral = W_(fused_scale(W_(tensor,u)), u)\n",
    "\n",
    "print(torch.allclose(spectral_then_fused_then_spectral, fused_scale_then_spectral))"
   ]
  },
  {
   "source": [
    "# OrderedDict experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from models.generative.DenseBlock import DenseBlock\n",
    "from models.generative.ResidualBlock import ResidualBlock\n",
    "from models.generative.ConvolutionalBlock import ConvolutionalBlock\n",
    "from models.generative.AttentionBlock import AttentionBlock\n",
    "\n",
    "pathgan_blocks = OrderedDict({\n",
    "            (\"dense_block_1\", DenseBlock) : 1,\n",
    "            (\"dense_block_2\", DenseBlock) : 2,\n",
    "            (\"res_block_1\", ResidualBlock) : 3,\n",
    "            (\"upscale_block_1\", ConvolutionalBlock) : 4,\n",
    "            (\"res_block_2\", ResidualBlock) : 5,\n",
    "            (\"upscale_block_2\", ConvolutionalBlock) : 6,\n",
    "            (\"res_block_2\", ResidualBlock) : 7,\n",
    "            (\"attention_block\", AttentionBlock) : 8,\n",
    "            (\"upscale_block_3\", ConvolutionalBlock) : 9,\n",
    "            (\"res_block_3\", ResidualBlock) : 10,\n",
    "            (\"upscale_layer_4\", ConvolutionalBlock) : 11,\n",
    "            (\"res_block_4\", ResidualBlock) : 12,\n",
    "            (\"upscale_block_4\", ConvolutionalBlock) : 13,\n",
    "            (\"sigmoid_block\", ConvolutionalBlock) : 14\n",
    "        })\n",
    "\n",
    "print(len(pathgan_blocks))\n",
    "\n",
    "pathgan_blocks_as_tuple = (\n",
    "            (\"dense_block_1\", DenseBlock),\n",
    "            (\"dense_block_2\", DenseBlock),\n",
    "            (\"res_block_1\", ResidualBlock),\n",
    "            (\"upscale_block_1\", ConvolutionalBlock),\n",
    "            (\"res_block_2\", ResidualBlock),\n",
    "            (\"upscale_block_2\", ConvolutionalBlock),\n",
    "            (\"res_block_2\", ResidualBlock),\n",
    "            (\"attention_block\", AttentionBlock),\n",
    "            (\"upscale_block_3\", ConvolutionalBlock),\n",
    "            (\"res_block_3\", ResidualBlock),\n",
    "            (\"upscale_layer_4\", ConvolutionalBlock),\n",
    "            (\"res_block_4\", ResidualBlock),\n",
    "            (\"upscale_block_4\", ConvolutionalBlock),\n",
    "            (\"sigmoid_block\", ConvolutionalBlock),\n",
    "        )\n",
    "\n",
    "print(len(pathgan_blocks_as_tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pathgan_blocks_as_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.spectral_norm import SpectralNorm, spectral_norm\n",
    "import copy\n",
    "\n",
    "in_channels, out_channels, kernel_size = 1, 1, 3\n",
    "test_module = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "nn.init.constant_(test_module.weight, 1)\n",
    "\n",
    "test_module2 = copy.deepcopy(test_module)\n",
    "\n",
    "spectral_test1 = spectral_norm(test_module)\n",
    "spectral_test2 = spectral_norm(test_module2)\n",
    "\n",
    "print(spectral_test1.weight)\n",
    "print(spectral_test2.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "def fused_scale(weight: Tensor) -> Tensor:\n",
    "        padded = F.pad(weight, [1, 1, 1, 1])\n",
    "        filter = padded[:, :, 1:, 1:] + padded[:, :, 1:, :-1] + padded[:, :, :-1, 1:] + padded[:, :, :-1, :-1]\n",
    "        return filter\n",
    "\n",
    "def _l2normalize(v, eps=1e-12):\n",
    "    return v / (torch.norm(v) + eps)\n",
    "\n",
    "def max_singular_value(W, u=None, Ip=1):\n",
    "    \"\"\"\n",
    "    power iteration for weight parameter\n",
    "    \"\"\"\n",
    "    #xp = W.data\n",
    "    if Ip < 1:\n",
    "        raise ValueError(\"Power iteration should be a positive integer\")\n",
    "    if u is None:\n",
    "        u = torch.FloatTensor(1, W.size(0)).normal_(0, 1).cpu()\n",
    "    _u = u\n",
    "    for _ in range(Ip):\n",
    "        _v = _l2normalize(torch.matmul(_u, W.data), eps=1e-12)\n",
    "        _u = _l2normalize(torch.matmul(_v, torch.transpose(W.data, 0, 1)), eps=1e-12)\n",
    "    sigma = torch.sum(F.linear(_u, torch.transpose(W.data, 0, 1)) * _v) # type: ignore\n",
    "    return sigma, _u\n",
    "\n",
    "def W_(weights, u):\n",
    "    w_mat = weights.view(weights.size(0), -1)\n",
    "    sigma, _u = max_singular_value(w_mat, u)\n",
    "    u.copy_(_u) # type: ignore\n",
    "    return weights / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensor = torch.rand(1,1,3,3)\n",
    "u = torch.Tensor(1, 1).normal_()\n",
    "\n",
    "fused_scale_then_spectral = W_(fused_scale(tensor), u)\n",
    "spectral_then_fused_scale = fused_scale(W_(tensor,u))\n",
    "spectral_then_fused_then_spectral = W_(fused_scale(W_(tensor,u)), u)\n",
    "\n",
    "print(torch.allclose(spectral_then_fused_then_spectral, fused_scale_then_spectral))"
   ]
  },
  {
   "source": [
    "# Generator class experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.generative.Generator import Generator\n",
    "\n",
    "generator = Generator()\n",
    "for child in generator.named_children():\n",
    "    print(type(child[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.blocks.ConvolutionalBlock import UpscaleBlock\n",
    "print(type(UpscaleBlock(1,1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'initialization'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fe832c86dc33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerative\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DAI/PathGan-pytorch/modules/generative/Generator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, latent_dim, output_shape, dense_out_channels, synthesis_out_channels, kernel_size, synthesis_block_with_attention, blocks_in_residual, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mnext_in_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_dense_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_in_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mnext_in_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_reshape_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_in_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mnext_in_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_synthesis_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_in_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_sigmoid_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_in_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DAI/PathGan-pytorch/modules/generative/Generator.py\u001b[0m in \u001b[0;36madd_synthesis_blocks\u001b[0;34m(self, in_channels, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mhas_attention_block\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynthesis_block_with_attention\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mout_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynthesis_out_channels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             self.add_synthesis_block(scope, in_channels, out_channels, self.kernel_size,\n\u001b[0m\u001b[1;32m    106\u001b[0m                                      has_attention_block, **kwargs)\n\u001b[1;32m    107\u001b[0m             \u001b[0min_channels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DAI/PathGan-pytorch/modules/generative/Generator.py\u001b[0m in \u001b[0;36madd_synthesis_block\u001b[0;34m(self, scope, in_channels, out_channels, kernel_size, has_attention_block, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"res_block_{scope}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResidualBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks_in_residual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual_block_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_attention_block\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"attention_block_{scope}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttentionBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"upscale_block_{scope}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUpscaleBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DAI/PathGan-pytorch/modules/blocks/AttentionBlock.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, channels, regularization, initialization, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mh_conv_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvolutionalBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_conv_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconv_block_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvolutionalBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_conv_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconv_block_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvolutionalBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_conv_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mconv_block_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DAI/PathGan-pytorch/modules/blocks/ConvolutionalBlock.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, conv_layer, layer_name, same_padding, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModuleDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mlayer_name\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mconv_layer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'initialization'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "from modules.blocks.DenseBlock import DenseBlock\n",
    "from modules.blocks.ResidualBlock import ResidualBlock\n",
    "from modules.generative.Generator import Generator\n",
    "\n",
    "generator = Generator()\n",
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.rand((10, 200))\n",
    "latent_in = torch.zeros((10, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"251.831987pt\" version=\"1.1\" viewBox=\"0 0 257.9275 251.831987\" width=\"257.9275pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-02-01T21:08:34.497712</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 251.831987 \nL 257.9275 251.831987 \nL 257.9275 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 227.953862 \nL 250.7275 227.953862 \nL 250.7275 10.513862 \nL 33.2875 10.513862 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pddf4b41505)\">\n    <image height=\"218\" id=\"image5973a9eddc\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"33.2875\" xlink:href=\"data:image/png;base64,\niVBORw0KGgoAAAANSUhEUgAAANoAAADaCAYAAADAHVzbAAACeElEQVR4nO3TMQHAIBDAQMD4Sy8qmg69U5Ale2aeBbzqfB0Af2A0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIGA0CBgNAkaDgNEgYDQIGA0CRoOA0SBgNAgYDQJGg4DRIHABbY0EMMBk7HMAAAAASUVORK5CYII=\" y=\"-9.953862\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m6c14080d34\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.772857\" xlink:href=\"#m6c14080d34\" y=\"227.953862\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(30.591607 242.552299)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"82.308571\" xlink:href=\"#m6c14080d34\" y=\"227.953862\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <g transform=\"translate(75.946071 242.552299)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"130.844286\" xlink:href=\"#m6c14080d34\" y=\"227.953862\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <g transform=\"translate(121.300536 242.552299)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"179.38\" xlink:href=\"#m6c14080d34\" y=\"227.953862\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 150 -->\n      <g transform=\"translate(169.83625 242.552299)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"227.915714\" xlink:href=\"#m6c14080d34\" y=\"227.953862\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 200 -->\n      <g transform=\"translate(218.371964 242.552299)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m76bf1947b9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m76bf1947b9\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m76bf1947b9\" y=\"35.267076\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 25 -->\n      <g transform=\"translate(13.5625 39.066295)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m76bf1947b9\" y=\"59.534933\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 63.334152)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m76bf1947b9\" y=\"83.80279\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 75 -->\n      <g transform=\"translate(13.5625 87.602009)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m76bf1947b9\" y=\"108.070647\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 111.869866)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m76bf1947b9\" y=\"132.338504\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 125 -->\n      <g transform=\"translate(7.2 136.137723)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m76bf1947b9\" y=\"156.606362\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 150 -->\n      <g transform=\"translate(7.2 160.40558)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m76bf1947b9\" y=\"180.874219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 175 -->\n      <g transform=\"translate(7.2 184.673437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m76bf1947b9\" y=\"205.142076\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 200 -->\n      <g transform=\"translate(7.2 208.941295)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 33.2875 227.953862 \nL 33.2875 10.513862 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 250.7275 227.953862 \nL 250.7275 10.513862 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 33.2875 227.953862 \nL 250.7275 227.953862 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 33.2875 10.513862 \nL 250.7275 10.513862 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pddf4b41505\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"33.2875\" y=\"10.513862\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOz0lEQVR4nO3df+xddX3H8edrKCRfNfxQIKXUUUg1g2Wr2rAlTOLGFCSLhSW6ksV0G1k1gUQTl6xosuESEudE/1nElEDWLYwfDpH+wTabxmhMpvyyFkqttFDlS79pFZdBVqNree+Pe77xUr5f+vX+6L3ffJ6P5Jt7zuecc+/75KSvnnPuzXmnqpDUrl+bdAGSJssQkBpnCEiNMwSkxhkCUuMMAalxYwuBJFcl2ZtkX5LN4/ocScPJOH4nkOQU4AfAe4FZ4BHguqp6auQfJmko4zoTuBTYV1XPVNUvgHuA9WP6LElDeN2Y3ncl8Fzf/CzwO4utPDMzU2ecccaYSpEEMDc395OqOvv48XGFQBYYe8V1R5JNwCaA008/nU2bNo2pFEkAn/70p3+40Pi4LgdmgVV98+cDB/tXqKotVbWuqtbNzMyMqQxJJzKuEHgEWJNkdZJTgQ3AtjF9lqQhjOVyoKqOJrkR+E/gFODOqto9js+SNJxx3ROgqh4CHhrX+0saDX8xKDXOEJAaZwhIjTMEpMYZAlLjDAGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNS4gUMgyaokX0+yJ8nuJB/rxm9O8nySnd3f1aMrV9KoDfNQkaPAJ6rq8SRvAh5Lsr1b9oWq+tzw5Ukat4FDoKrmgLlu+qUke+g9alzSMjKSewJJLgDeAXynG7oxya4kdyY5cxSfIWk8hg6BJG8E7gc+XlUvArcBFwFr6Z0p3LrIdpuSPJrk0SNHjgxbhqQBDRUCSV5PLwDuqqqvAFTVoao6VlUvA7fTa0n2KvYdkKbDMN8OBLgD2FNVn+8bX9G32rXAk4OXJ2nchvl24DLgw8ATSXZ2Y58Erkuyll7bsQPAR4b4DEljNsy3A99i4Z6D9hqQlhF/MSg1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxhoDUOENAatwwTxYiyQHgJeAYcLSq1iU5C7gXuIDek4U+VFX/PVyZksZlFGcCv19Va6tqXTe/GdhRVWuAHd28pCk1jsuB9cDWbnorcM0YPkPSiAwbAgV8LcljSTZ1Y+d23YnmuxSds9CG9h2QpsNQ9wSAy6rqYJJzgO1Jvr/UDatqC7AF4Lzzzqsh65A0oKHOBKrqYPd6GHiAXqORQ/O9B7rXw8MWKWl8hmk+8oauGzFJ3gC8j16jkW3Axm61jcCDwxYpaXyGuRw4F3ig14iI1wH/WlX/keQR4L4k1wM/Aj44fJmSxmWY5iPPAL+9wPgLwBXDFCXp5PEXg1LjDAGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNG/h5AkneTq+/wLwLgb8BzgD+EvhxN/7Jqnpo0M+RNF7DPFRkL7AWIMkpwPP0njP458AXqupzoyhQ0niN6nLgCmB/Vf1wRO8n6SQZVQhsAO7um78xya4kdyY5c0SfIWkMhg6BJKcCHwC+3A3dBlxE71JhDrh1ke1sPiJNgVGcCbwfeLyqDgFU1aGqOlZVLwO30+tF8CpVtaWq1lXVupmZmRGUIWkQowiB6+i7FJhvPNK5ll4vAklTatjW5DPAe4GP9A1/Nslaen0KDxy3TNKUGSoEquoI8Objxj48VEWSTip/MSg1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxhoDUuBOGQPew0MNJnuwbOyvJ9iRPd69n9i27Kcm+JHuTXDmuwiWNxlLOBP4JuOq4sc3AjqpaA+zo5klyMb0nD1/SbfPFrieBpCl1whCoqm8CPz1ueD2wtZveClzTN35PVf28qp4F9rHIg0YlTYdB7wmcW1VzAN3rOd34SuC5vvVmuzFJU2rUNwazwFgtuKJ9B6SpMGgIHJp/tHj3ergbnwVW9a13PnBwoTew74A0HQYNgW3Axm56I/Bg3/iGJKclWQ2sAR4erkRJ43TCR44nuRt4D/CWJLPA3wKfAe5Lcj3wI+CDAFW1O8l9wFPAUeCGqjo2ptoljcAJQ6Cqrltk0RWLrH8LcMswRUk6efzFoNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxhoDUOENAapwhIDXOEJAaZwhIjTMEpMYZAlLjDAGpcYM2H/mHJN9PsivJA0nO6MYvSPKzJDu7vy+NsXZJIzBo85HtwG9W1W8BPwBu6lu2v6rWdn8fHU2ZksZloOYjVfW1qjrazX6b3lOFJS1Do7gn8BfAv/fNr07y3STfSPLuxTay74A0HU74oNHXkuRT9J4qfFc3NAe8tapeSPIu4KtJLqmqF4/ftqq2AFsAzjvvvAUblEgav4HPBJJsBP4I+NOqKoCuB+EL3fRjwH7gbaMoVNJ4DBQCSa4C/hr4QFUd6Rs/e74LcZIL6TUfeWYUhUoaj0Gbj9wEnAZsTwLw7e6bgMuBv0tyFDgGfLSqju9oLGmKDNp85I5F1r0fuH/YoiSdPP5iUGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxg/YduDnJ8339Ba7uW3ZTkn1J9ia5clyFSxqNQfsOAHyhr7/AQwBJLgY2AJd023xx/nFjkqbTQH0HXsN64J7ugaPPAvuAS4eoT9KYDXNP4MauDdmdSc7sxlYCz/WtM9uNvYp9B6TpMGgI3AZcBKyl12vg1m48C6y7YE+BqtpSVeuqat3MzMyAZUga1kAhUFWHqupYVb0M3M4vT/lngVV9q54PHByuREnjNGjfgRV9s9cC898cbAM2JDktyWp6fQceHq5ESeM0aN+B9yRZS+9U/wDwEYCq2p3kPuApeu3JbqiqY2OpXNJIjLTvQLf+LcAtwxQl6eTxF4NS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxhoDUOENAapwhIDXOEJAaZwhIjRu078C9fT0HDiTZ2Y1fkORnfcu+NMbaJY3ACR8qQq/vwD8C/zw/UFV/Mj+d5Fbgf/rW319Va0dUn6QxW8qThb6Z5IKFliUJ8CHgD0Zcl6STZNh7Au8GDlXV031jq5N8N8k3krx7yPeXNGZLuRx4LdcBd/fNzwFvraoXkrwL+GqSS6rqxeM3TLIJ2ARw+umnD1mGpEENfCaQ5HXAHwP3zo917cde6KYfA/YDb1toe5uPSNNhmMuBPwS+X1Wz8wNJzp5vQJrkQnp9B54ZrkRJ47SUrwjvBv4LeHuS2STXd4s28MpLAYDLgV1Jvgf8G/DRqlpqM1NJEzBo3wGq6s8WGLsfuH/4siSdLP5iUGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxhkCUuMMAalxS3moyKokX0+yJ8nuJB/rxs9Ksj3J093rmX3b3JRkX5K9Sa4c5w5IGs5SzgSOAp+oqt8Afhe4IcnFwGZgR1WtAXZ083TLNgCXAFcBX5x/5Jik6XPCEKiquap6vJt+CdgDrATWA1u71bYC13TT64F7uoeOPgvsAy4dcd2SRuRXuifQNSF5B/Ad4NyqmoNeUADndKutBJ7r22y2G5M0hZYcAkneSO/5gR9fqI9A/6oLjNUC77cpyaNJHj1y5MhSy5A0YksKgSSvpxcAd1XVV7rhQ0lWdMtXAIe78VlgVd/m5wMHj39P+w5I02Ep3w4EuAPYU1Wf71u0DdjYTW8EHuwb35DktCSr6fUeeHh0JUsapaW0IbsM+DDwxHwLcuCTwGeA+7o+BD8CPghQVbuT3Ac8Re+bhRuq6tioC5c0GkvpO/AtFr7OB7hikW1uAW4Zoi5JJ4m/GJQaZwhIjTMEpMYZAlLjDAGpcYaA1DhDQGqcISA1zhCQGmcISI0zBKTGGQJS4wwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNS9WrngZ+8otIfgz8L/CTSdcyhLewvOuH5b8Py71+GO8+/HpVnX384FSEAECSR6tq3aTrGNRyrx+W/z4s9/phMvvg5YDUOENAatw0hcCWSRcwpOVePyz/fVju9cME9mFq7glImoxpOhOQNAETD4EkVyXZm2Rfks2TrmepkhxI8kSSnUke7cbOSrI9ydPd65mTrnNekjuTHE7yZN/YovUmuak7JnuTXDmZql9pkX24Ocnz3XHYmeTqvmVTtQ9JViX5epI9SXYn+Vg3PtnjUFUT+wNOAfYDFwKnAt8DLp5kTb9C7QeAtxw39llgcze9Gfj7SdfZV9vlwDuBJ09UL3BxdyxOA1Z3x+iUKd2Hm4G/WmDdqdsHYAXwzm76TcAPujonehwmfSZwKbCvqp6pql8A9wDrJ1zTMNYDW7vprcA1kyvllarqm8BPjxterN71wD1V9fOqehbYR+9YTdQi+7CYqduHqpqrqse76ZeAPcBKJnwcJh0CK4Hn+uZnu7HloICvJXksyaZu7NyqmoPeAQfOmVh1S7NYvcvtuNyYZFd3uTB/Kj3V+5DkAuAdwHeY8HGYdAgs1O14uXxdcVlVvRN4P3BDkssnXdAILafjchtwEbAWmANu7candh+SvBG4H/h4Vb34WqsuMDbyfZh0CMwCq/rmzwcOTqiWX0lVHexeDwMP0DtNO5RkBUD3enhyFS7JYvUum+NSVYeq6lhVvQzczi9Pl6dyH5K8nl4A3FVVX+mGJ3ocJh0CjwBrkqxOciqwAdg24ZpOKMkbkrxpfhp4H/Akvdo3dqttBB6cTIVLtli924ANSU5LshpYAzw8gfpOaP4fT+daescBpnAfkgS4A9hTVZ/vWzTZ4zAFd3yvpneXdD/wqUnXs8SaL6R31/Z7wO75uoE3AzuAp7vXsyZda1/Nd9M7Xf4/ev/DXP9a9QKf6o7JXuD9k67/NfbhX4AngF3dP5oV07oPwO/RO53fBezs/q6e9HHwF4NS4yZ9OSBpwgwBqXGGgNQ4Q0BqnCEgNc4QkBpnCEiNMwSkxv0/Lhk7af24ZUYAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "out = generator(data, latent_in)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def view_image(tensor, index):\n",
    "    permuted = tensor[index].permute(1, 2, 0)\n",
    "    plt.imshow(permuted.detach().numpy())\n",
    "\n",
    "view_image(out, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dense_block_0.dense_layer.bias \n\ndense_block_0.dense_layer.weight_orig \n\ndense_block_0.noise_input.weight \n\ndense_block_0.normalization.dense_layer.bias \n\ndense_block_0.normalization.dense_layer.weight_orig \n\ndense_block_0.normalization.gamma_layer.bias \n\ndense_block_0.normalization.gamma_layer.weight_orig \n\ndense_block_0.normalization.beta_layer.bias \n\ndense_block_0.normalization.beta_layer.weight_orig \n\ndense_block_1.dense_layer.bias \n\ndense_block_1.dense_layer.weight_orig \n\ndense_block_1.noise_input.weight \n\ndense_block_1.normalization.dense_layer.bias \n\ndense_block_1.normalization.dense_layer.weight_orig \n\ndense_block_1.normalization.gamma_layer.bias \n\ndense_block_1.normalization.gamma_layer.weight_orig \n\ndense_block_1.normalization.beta_layer.bias \n\ndense_block_1.normalization.beta_layer.weight_orig \n\nres_block_0.part_1.conv_layer.bias \n\nres_block_0.part_1.conv_layer.weight_orig \n\nres_block_0.part_1.noise_input.weight \n\nres_block_0.part_1.normalization.dense_layer.bias \n\nres_block_0.part_1.normalization.dense_layer.weight_orig \n\nres_block_0.part_1.normalization.gamma_layer.bias \n\nres_block_0.part_1.normalization.gamma_layer.weight_orig \n\nres_block_0.part_1.normalization.beta_layer.bias \n\nres_block_0.part_1.normalization.beta_layer.weight_orig \n\nres_block_0.part_2.conv_layer.bias \n\nres_block_0.part_2.conv_layer.weight_orig \n\nres_block_0.part_2.noise_input.weight \n\nres_block_0.part_2.normalization.dense_layer.bias \n\nres_block_0.part_2.normalization.dense_layer.weight_orig \n\nres_block_0.part_2.normalization.gamma_layer.bias \n\nres_block_0.part_2.normalization.gamma_layer.weight_orig \n\nres_block_0.part_2.normalization.beta_layer.bias \n\nres_block_0.part_2.normalization.beta_layer.weight_orig \n\nupscale_block_0.upscale_layer.bias \n\nupscale_block_0.upscale_layer.weight_orig \n\nupscale_block_0.noise_input.weight \n\nupscale_block_0.normalization.dense_layer.bias \n\nupscale_block_0.normalization.dense_layer.weight_orig \n\nupscale_block_0.normalization.gamma_layer.bias \n\nupscale_block_0.normalization.gamma_layer.weight_orig \n\nupscale_block_0.normalization.beta_layer.bias \n\nupscale_block_0.normalization.beta_layer.weight_orig \n\nres_block_1.part_1.conv_layer.bias \n\nres_block_1.part_1.conv_layer.weight_orig \n\nres_block_1.part_1.noise_input.weight \n\nres_block_1.part_1.normalization.dense_layer.bias \n\nres_block_1.part_1.normalization.dense_layer.weight_orig \n\nres_block_1.part_1.normalization.gamma_layer.bias \n\nres_block_1.part_1.normalization.gamma_layer.weight_orig \n\nres_block_1.part_1.normalization.beta_layer.bias \n\nres_block_1.part_1.normalization.beta_layer.weight_orig \n\nres_block_1.part_2.conv_layer.bias \n\nres_block_1.part_2.conv_layer.weight_orig \n\nres_block_1.part_2.noise_input.weight \n\nres_block_1.part_2.normalization.dense_layer.bias \n\nres_block_1.part_2.normalization.dense_layer.weight_orig \n\nres_block_1.part_2.normalization.gamma_layer.bias \n\nres_block_1.part_2.normalization.gamma_layer.weight_orig \n\nres_block_1.part_2.normalization.beta_layer.bias \n\nres_block_1.part_2.normalization.beta_layer.weight_orig \n\nupscale_block_1.upscale_layer.bias \n\nupscale_block_1.upscale_layer.weight_orig \n\nupscale_block_1.noise_input.weight \n\nupscale_block_1.normalization.dense_layer.bias \n\nupscale_block_1.normalization.dense_layer.weight_orig \n\nupscale_block_1.normalization.gamma_layer.bias \n\nupscale_block_1.normalization.gamma_layer.weight_orig \n\nupscale_block_1.normalization.beta_layer.bias \n\nupscale_block_1.normalization.beta_layer.weight_orig \n\nres_block_2.part_1.conv_layer.bias \n\nres_block_2.part_1.conv_layer.weight_orig \n\nres_block_2.part_1.noise_input.weight \n\nres_block_2.part_1.normalization.dense_layer.bias \n\nres_block_2.part_1.normalization.dense_layer.weight_orig \n\nres_block_2.part_1.normalization.gamma_layer.bias \n\nres_block_2.part_1.normalization.gamma_layer.weight_orig \n\nres_block_2.part_1.normalization.beta_layer.bias \n\nres_block_2.part_1.normalization.beta_layer.weight_orig \n\nres_block_2.part_2.conv_layer.bias \n\nres_block_2.part_2.conv_layer.weight_orig \n\nres_block_2.part_2.noise_input.weight \n\nres_block_2.part_2.normalization.dense_layer.bias \n\nres_block_2.part_2.normalization.dense_layer.weight_orig \n\nres_block_2.part_2.normalization.gamma_layer.bias \n\nres_block_2.part_2.normalization.gamma_layer.weight_orig \n\nres_block_2.part_2.normalization.beta_layer.bias \n\nres_block_2.part_2.normalization.beta_layer.weight_orig \n\nattention_block_2.gamma \n\nattention_block_2.attention_f.conv_layer.weight \n\nattention_block_2.attention_f.conv_layer.bias \n\nattention_block_2.attention_g.conv_layer.weight \n\nattention_block_2.attention_g.conv_layer.bias \n\nattention_block_2.attention_h.conv_layer.weight \n\nattention_block_2.attention_h.conv_layer.bias \n\nupscale_block_2.upscale_layer.bias \n\nupscale_block_2.upscale_layer.weight_orig \n\nupscale_block_2.noise_input.weight \n\nupscale_block_2.normalization.dense_layer.bias \n\nupscale_block_2.normalization.dense_layer.weight_orig \n\nupscale_block_2.normalization.gamma_layer.bias \n\nupscale_block_2.normalization.gamma_layer.weight_orig \n\nupscale_block_2.normalization.beta_layer.bias \n\nupscale_block_2.normalization.beta_layer.weight_orig \n\nres_block_3.part_1.conv_layer.bias \n\nres_block_3.part_1.conv_layer.weight_orig \n\nres_block_3.part_1.noise_input.weight \n\nres_block_3.part_1.normalization.dense_layer.bias \n\nres_block_3.part_1.normalization.dense_layer.weight_orig \n\nres_block_3.part_1.normalization.gamma_layer.bias \n\nres_block_3.part_1.normalization.gamma_layer.weight_orig \n\nres_block_3.part_1.normalization.beta_layer.bias \n\nres_block_3.part_1.normalization.beta_layer.weight_orig \n\nres_block_3.part_2.conv_layer.bias \n\nres_block_3.part_2.conv_layer.weight_orig \n\nres_block_3.part_2.noise_input.weight \n\nres_block_3.part_2.normalization.dense_layer.bias \n\nres_block_3.part_2.normalization.dense_layer.weight_orig \n\nres_block_3.part_2.normalization.gamma_layer.bias \n\nres_block_3.part_2.normalization.gamma_layer.weight_orig \n\nres_block_3.part_2.normalization.beta_layer.bias \n\nres_block_3.part_2.normalization.beta_layer.weight_orig \n\nupscale_block_3.upscale_layer.bias \n\nupscale_block_3.upscale_layer.weight_orig \n\nupscale_block_3.noise_input.weight \n\nupscale_block_3.normalization.dense_layer.bias \n\nupscale_block_3.normalization.dense_layer.weight_orig \n\nupscale_block_3.normalization.gamma_layer.bias \n\nupscale_block_3.normalization.gamma_layer.weight_orig \n\nupscale_block_3.normalization.beta_layer.bias \n\nupscale_block_3.normalization.beta_layer.weight_orig \n\nres_block_4.part_1.conv_layer.bias \n\nres_block_4.part_1.conv_layer.weight_orig \n\nres_block_4.part_1.noise_input.weight \n\nres_block_4.part_1.normalization.dense_layer.bias \n\nres_block_4.part_1.normalization.dense_layer.weight_orig \n\nres_block_4.part_1.normalization.gamma_layer.bias \n\nres_block_4.part_1.normalization.gamma_layer.weight_orig \n\nres_block_4.part_1.normalization.beta_layer.bias \n\nres_block_4.part_1.normalization.beta_layer.weight_orig \n\nres_block_4.part_2.conv_layer.bias \n\nres_block_4.part_2.conv_layer.weight_orig \n\nres_block_4.part_2.noise_input.weight \n\nres_block_4.part_2.normalization.dense_layer.bias \n\nres_block_4.part_2.normalization.dense_layer.weight_orig \n\nres_block_4.part_2.normalization.gamma_layer.bias \n\nres_block_4.part_2.normalization.gamma_layer.weight_orig \n\nres_block_4.part_2.normalization.beta_layer.bias \n\nres_block_4.part_2.normalization.beta_layer.weight_orig \n\nupscale_block_4.upscale_layer.bias \n\nupscale_block_4.upscale_layer.weight_orig \n\nupscale_block_4.noise_input.weight \n\nupscale_block_4.normalization.dense_layer.bias \n\nupscale_block_4.normalization.dense_layer.weight_orig \n\nupscale_block_4.normalization.gamma_layer.bias \n\nupscale_block_4.normalization.gamma_layer.weight_orig \n\nupscale_block_4.normalization.beta_layer.bias \n\nupscale_block_4.normalization.beta_layer.weight_orig \n\nsigmoid_block.conv_layer.bias \n\nsigmoid_block.conv_layer.weight_orig \n\nsigmoid_block.noise_input.weight \n\nsigmoid_block.normalization.dense_layer.bias \n\nsigmoid_block.normalization.dense_layer.weight_orig \n\nsigmoid_block.normalization.gamma_layer.bias \n\nsigmoid_block.normalization.gamma_layer.weight_orig \n\nsigmoid_block.normalization.beta_layer.bias \n\nsigmoid_block.normalization.beta_layer.weight_orig \n\n"
     ]
    }
   ],
   "source": [
    "for np in generator.named_parameters():\n",
    "    print(np[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "output_image_size = (224,256)\n",
    "number_of_synthesis_blocks = 5\n",
    "        \n",
    "upscale_factor = 2 ** number_of_synthesis_blocks\n",
    "image_size_valid = all(side % upscale_factor == 0 for side in output_image_size)\n",
    "image_size_valid"
   ]
  },
  {
   "source": [
    "# Reshape experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([10, 256, 7, 7])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "input = torch.rand(10, 12544)\n",
    "output = torch.reshape(input, (10, 256, 7, 7))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}