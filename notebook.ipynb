{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pathgan-torch",
   "display_name": "Python 3.8 - pathgan-torch",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Kernel padding experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_channels = 1\n",
    "in_channels = 1\n",
    "kernel_size = (2,2)\n",
    "weights = torch.ones(out_channels,in_channels, kernel_size[0], kernel_size[1])\n",
    "print(weights.shape)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = F.pad(weights, [1, 1, 1, 1])\n",
    "print(padded.shape)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_upscale = padded[:, :, 1:, 1:] + padded[:, :, 1:, :-1] + padded[:, :, :-1, 1:] + padded[:, :, :-1, :-1]\n",
    "\n",
    "print(padded[:, :, 1:, 1:], '\\n\\n', padded[:, :, 1:, :-1], '\\n\\n', padded[:, :, :-1, 1:], '\\n\\n', padded[:, :, :-1, :-1])\n",
    "\n",
    "print(padded_upscale.shape)\n",
    "print(padded_upscale /4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mat = weights.view(weights.size(0), -1)\n",
    "w_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import max_singular_value\n",
    "u = torch.FloatTensor(1, w_mat.size(0)).normal_(0, 1).cpu()\n",
    "sigma, _u = max_singular_value(w_mat, u, 1)\n",
    "sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_upscale/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = weights[:, :, 1:, 1:] + weights[:, :, 1:, :-\n",
    "                                            1] + weights[:, :, :-1, 1:] + weights[:, :, :-1, :-1]\n",
    "w_mat = weights.view(weights.size(0), -1)\n",
    "sigma, _u = max_singular_value(w_mat, self.u, 1)\n",
    "self.u.copy_(_u)\n",
    "return weights / sigma"
   ]
  },
  {
   "source": [
    "# _output_padding experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def _output_padding(input, output_padding, output_size, stride, padding, kernel_size, dilation=None):\n",
    "        # type: (Tensor, Optional[List[int]], List[int], List[int], List[int], Optional[List[int]]) -> List[int]\n",
    "        if output_size is None:\n",
    "            ret = tuple(output_padding)  # converting to list if was not already\n",
    "        else:\n",
    "            k = input.dim() - 2\n",
    "            if len(output_size) == k + 2:\n",
    "                output_size = output_size[2:]\n",
    "            if len(output_size) != k:\n",
    "                raise ValueError(\n",
    "                    \"output_size must have {} or {} elements (got {})\"\n",
    "                    .format(k, k + 2, len(output_size)))\n",
    "\n",
    "            min_sizes = torch.jit.annotate(List[int], [])\n",
    "            max_sizes = torch.jit.annotate(List[int], [])\n",
    "            for d in range(k):\n",
    "                dim_size = ((input.size(d + 2) - 1) * stride[d] -\n",
    "                            2 * padding[d] +\n",
    "                            (dilation[d] if dilation is not None else 1) * (kernel_size[d] - 1) + 1)\n",
    "                min_sizes.append(dim_size)\n",
    "                max_sizes.append(min_sizes[d] + stride[d] - 1)\n",
    "\n",
    "            for i in range(len(output_size)):\n",
    "                size = output_size[i]\n",
    "                min_size = min_sizes[i]\n",
    "                max_size = max_sizes[i]\n",
    "                if size < min_size or size > max_size:\n",
    "                    raise ValueError((\n",
    "                        \"requested an output size of {}, but valid sizes range \"\n",
    "                        \"from {} to {} (for an input of {})\").format(\n",
    "                            output_size, min_sizes, max_sizes, input.size()[2:]))\n",
    "\n",
    "            res = torch.jit.annotate(List[int], [])\n",
    "            for d in range(k):\n",
    "                res.append(output_size[d] - min_sizes[d])\n",
    "\n",
    "            ret = res\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = torch.rand((64, 3, 224, 224))\n",
    "\n",
    "output_padding = _output_padding(inputs, output_padding=[0,0], output_size=[input_x * 2,input_y * 2], stride=[2,2], padding=[1,1], kernel_size=[3,3], dilation=None)\n",
    "\n",
    "output_padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_maintain_size(dilation, kernel_size):\n",
    "    effective_filter_size = dilation * (kernel_size - 1) + 1\n",
    "    if effective_filter_size % 2 == 0:\n",
    "        print(\"In order to maintain input image size, effective filter size must be odd\")\n",
    "    return (effective_filter_size - 1) // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for stride in range(1,4):\n",
    "        for dilation in range(1,3):\n",
    "            for kernel_size in range(1,6, 2):\n",
    "                failure = False\n",
    "                sizes = []\n",
    "\n",
    "                padding = pad_to_maintain_size(dilation, kernel_size)\n",
    "\n",
    "                for input_size in range(0, 100):\n",
    "                    output_size = input_size * 2\n",
    "                    min_size = (input_size - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + 1\n",
    "                    max_size = min_size + stride - 1\n",
    "                    if output_size < min_size or output_size > max_size:\n",
    "                        failure = True\n",
    "                        sizes.append(input_x)\n",
    "\n",
    "                message = 'fail' if failure else 'success'\n",
    "                print(f'{message} stride: {stride}, padding: {padding}, dilation: {dilation}, kernel_size: {kernel_size}, failed sizes: {len(sizes)}')\n",
    "        \n",
    "       "
   ]
  },
  {
   "source": [
    "from https://stats.stackexchange.com/questions/297678/how-to-calculate-optimal-zero-padding-for-convolutional-neural-networks\n",
    "\n",
    "The possible values for the padding size, ð‘ƒ, depends the input size ð‘Š (following the notation of the blog), the filter size ð¹ and the stride ð‘†. We assume width and height are the same.\n",
    "\n",
    "What you need to ensure is that the output size, (ð‘Šâˆ’ð¹+2ð‘ƒ)/ð‘†+1, is an integer. When ð‘†=1 then you get your first equation ð‘ƒ=(ð¹âˆ’1)/2 as necessary condition. But, in general, you need to consider the three parameters, namely ð‘Š, ð¹ and ð‘† in order to determine valid values of ð‘ƒ.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Same padding experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import Union\n",
    "\n",
    "def same_padding(conv_layer: Union[nn.Conv2d, nn.ConvTranspose2d], inputs: Tensor):\n",
    "    in_height = inputs.size(2)\n",
    "    in_width = inputs.size(3)\n",
    "    dilation = conv_layer.dilation\n",
    "    kernel_size = conv_layer.kernel_size\n",
    "    filter_size = (dilation[0] * (kernel_size[0] - 1) + 1, dilation[1] * (kernel_size[1] - 1) + 1)\n",
    "\n",
    "    print((in_height, in_width))\n",
    "    print(filter_size)\n",
    "\n",
    "    pad_along_height = dilation[0] * (kernel_size[0] - 1)\n",
    "    pad_along_width = dilation[1] * (kernel_size[1] - 1)\n",
    "\n",
    "    pad_top = pad_along_height // 2\n",
    "    pad_bottom = pad_along_height - pad_top\n",
    "    pad_left = pad_along_width // 2\n",
    "    pad_right = pad_along_width - pad_left\n",
    "\n",
    "    print(f'height: {pad_along_height}, width: {pad_along_width}')\n",
    "    print(f'bottom: {pad_bottom}, top: {pad_top}, left: {pad_left}, right: {pad_right}')\n",
    "\n",
    "    return F.pad(inputs, (pad_left, pad_right, pad_top, pad_bottom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand((64, 3, 122, 51))\n",
    "\n",
    "conv_layer = nn.Conv2d(in_channels=3,out_channels=3,kernel_size=(3,5), stride=1, dilation=1)\n",
    "\n",
    "padded_inputs = same_padding(conv_layer, inputs)\n",
    "output = conv_layer(padded_inputs)\n",
    "\n",
    "print(inputs.size())\n",
    "print(padded_inputs.size())\n",
    "print(output.size())"
   ]
  },
  {
   "source": [
    "# AdaIN experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple\n",
    "import unittest\n",
    "import torch\n",
    "from torch.tensor import Tensor\n",
    "import torch.nn as nn\n",
    "from models.generative.normalization.AdaptiveInstanceNormalization import AdaptiveInstanceNormalization\n",
    "\n",
    "class TestAdaIN():\n",
    "    #unit tests from https://zhangruochi.com/Components-of-StyleGAN/2020/10/13/\n",
    "    def setUp(self) -> None:\n",
    "        self.w_channels = 3\n",
    "        self.intermediate_channels = None\n",
    "        self.image_channels = 2\n",
    "        image_size = 3\n",
    "        n_test = 1\n",
    "        self.input_shape = (n_test, self.image_channels, image_size, image_size)\n",
    "        self.test_input = torch.ones(self.input_shape)\n",
    "        self.test_w = torch.ones(n_test, self.w_channels)\n",
    "        self.adain = AdaptiveInstanceNormalization(self.image_channels, self.w_channels, self.intermediate_channels)\n",
    "\n",
    "        nn.init.constant_(self.adain.gamma_layer.weight, 0.25)\n",
    "        nn.init.constant_(self.adain.beta_layer.weight, 0.2)\n",
    "        \n",
    "        for bias in (self.adain.gamma_layer.bias, self.adain.beta_layer.bias):\n",
    "            nn.init.zeros_(bias)\n",
    "            \n",
    "        if self.adain.dense_layer is not None:\n",
    "            nn.init.constant_(self.adain.dense_layer.weight, 0.2)\n",
    "            nn.init.zeros_(self.adain.dense_layer.bias)\n",
    "\n",
    "        self.test_w = torch.ones(n_test, self.w_channels)\n",
    "        \n",
    "    def test_layer_shape_correct(self):\n",
    "        assert self.adain.gamma_layer(self.test_w).shape == self.adain.beta_layer(self.test_w).shape\n",
    "        assert self.adain.gamma_layer(self.test_w).shape[-1] == self.image_channels\n",
    "        assert tuple(self.adain(self.test_input, self.test_w).shape) == self.input_shape\n",
    "    \n",
    "    def test_conditional_norm(self) -> None:\n",
    "        self.test_input[:, :, 0] = 0\n",
    "        test_output = self.adain(self.test_input, self.test_w)\n",
    "        assert(torch.abs(test_output[0, 0, 0, 0] - 3 / 5 + torch.sqrt(torch.tensor(9 / 8))) < 1e-4)\n",
    "        assert(torch.abs(test_output[0, 0, 1, 0] - 3 / 5 - torch.sqrt(torch.tensor(9 / 32))) < 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adain = TestAdaIN()\n",
    "test_adain.setUp()\n",
    "test_input = test_adain.test_input\n",
    "test_input[:,:,0] = 0\n",
    "test_w = test_adain.test_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = test_adain.adain(test_input, test_w)"
   ]
  },
  {
   "source": [
    "# (more) Spectral Norm experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import unittest\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.utils import spectral_norm \n",
    "from torch.nn.utils.spectral_norm import SpectralNorm\n",
    "from torch.nn.init import constant_\n",
    "\n",
    "from models.generative.ConvolutionalBlock import ConvolutionalBlock\n",
    "from models.generative.ConvolutionalScale import DownscaleConv2d, FusedScale, UpscaleConv2d\n",
    "\n",
    "parameter_name = 'weight'\n",
    "\n",
    "input_shape = (1, 3, 6, 6)\n",
    "in_channels, out_channels, kernel_size = 3, 6, 3\n",
    "fn = lambda x: spectral_norm(x, name = parameter_name, dim = 0)\n",
    "scale = fn(DownscaleConv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, fused_scale=True))\n",
    "nn.init.constant_(scale.weight, 1)\n",
    "dummy1 = copy.deepcopy(scale)\n",
    "dummy2 = copy.deepcopy(scale)\n",
    "\n",
    "data = torch.ones(input_shape)\n",
    "\n",
    "FusedScale(name=parameter_name)(dummy1, None)\n",
    "SpectralNorm(name=parameter_name, dim=0)(dummy1, None)\n",
    "dummy1_out = F.conv2d(data, dummy1.filter, dummy1.bias, stride=2, padding=dummy1.padding)\n",
    "\n",
    "SpectralNorm(name=parameter_name, dim=0)(dummy2, None)\n",
    "FusedScale(name=parameter_name)(dummy2, None)\n",
    "dummy2_out = F.conv2d(data, dummy2.filter, dummy2.bias, stride=2, padding=dummy2.padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "torch.equal(dummy1_out, dummy2_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\ntensor([[[[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]], requires_grad=True)\nParameter containing:\ntensor([[[[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.spectral_norm import SpectralNorm, spectral_norm\n",
    "import copy\n",
    "\n",
    "in_channels, out_channels, kernel_size = 1, 1, 3\n",
    "test_module = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "nn.init.constant_(test_module.weight, 1)\n",
    "\n",
    "test_module2 = copy.deepcopy(test_module)\n",
    "\n",
    "spectral_test1 = spectral_norm(test_module)\n",
    "spectral_test2 = spectral_norm(test_module2)\n",
    "\n",
    "print(spectral_test1.weight)\n",
    "print(spectral_test2.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "def fused_scale(weight: Tensor) -> Tensor:\n",
    "        padded = F.pad(weight, [1, 1, 1, 1])\n",
    "        filter = padded[:, :, 1:, 1:] + padded[:, :, 1:, :-1] + padded[:, :, :-1, 1:] + padded[:, :, :-1, :-1]\n",
    "        return filter\n",
    "\n",
    "def _l2normalize(v, eps=1e-12):\n",
    "    return v / (torch.norm(v) + eps)\n",
    "\n",
    "def max_singular_value(W, u=None, Ip=1):\n",
    "    \"\"\"\n",
    "    power iteration for weight parameter\n",
    "    \"\"\"\n",
    "    #xp = W.data\n",
    "    if Ip < 1:\n",
    "        raise ValueError(\"Power iteration should be a positive integer\")\n",
    "    if u is None:\n",
    "        u = torch.FloatTensor(1, W.size(0)).normal_(0, 1).cpu()\n",
    "    _u = u\n",
    "    for _ in range(Ip):\n",
    "        _v = _l2normalize(torch.matmul(_u, W.data), eps=1e-12)\n",
    "        _u = _l2normalize(torch.matmul(_v, torch.transpose(W.data, 0, 1)), eps=1e-12)\n",
    "    sigma = torch.sum(F.linear(_u, torch.transpose(W.data, 0, 1)) * _v) # type: ignore\n",
    "    return sigma, _u\n",
    "\n",
    "def W_(weights, u):\n",
    "    w_mat = weights.view(weights.size(0), -1)\n",
    "    sigma, _u = max_singular_value(w_mat, u)\n",
    "    u.copy_(_u) # type: ignore\n",
    "    return weights / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor = torch.rand(1,1,3,3)\n",
    "u = torch.Tensor(1, 1).normal_()\n",
    "\n",
    "fused_scale_then_spectral = W_(fused_scale(tensor), u)\n",
    "spectral_then_fused_scale = fused_scale(W_(tensor,u))\n",
    "spectral_then_fused_then_spectral = W_(fused_scale(W_(tensor,u)), u)\n",
    "\n",
    "print(torch.allclose(spectral_then_fused_then_spectral, fused_scale_then_spectral))"
   ]
  },
  {
   "source": [
    "# OrderedDict experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13\n14\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from models.generative.DenseBlock import DenseBlock\n",
    "from models.generative.ResidualBlock import ResidualBlock\n",
    "from models.generative.ConvolutionalBlock import ConvolutionalBlock\n",
    "from models.generative.AttentionBlock import AttentionBlock\n",
    "\n",
    "pathgan_blocks = OrderedDict({\n",
    "            (\"dense_block_1\", DenseBlock) : 1,\n",
    "            (\"dense_block_2\", DenseBlock) : 2,\n",
    "            (\"res_block_1\", ResidualBlock) : 3,\n",
    "            (\"upscale_block_1\", ConvolutionalBlock) : 4,\n",
    "            (\"res_block_2\", ResidualBlock) : 5,\n",
    "            (\"upscale_block_2\", ConvolutionalBlock) : 6,\n",
    "            (\"res_block_2\", ResidualBlock) : 7,\n",
    "            (\"attention_block\", AttentionBlock) : 8,\n",
    "            (\"upscale_block_3\", ConvolutionalBlock) : 9,\n",
    "            (\"res_block_3\", ResidualBlock) : 10,\n",
    "            (\"upscale_layer_4\", ConvolutionalBlock) : 11,\n",
    "            (\"res_block_4\", ResidualBlock) : 12,\n",
    "            (\"upscale_block_4\", ConvolutionalBlock) : 13,\n",
    "            (\"sigmoid_block\", ConvolutionalBlock) : 14\n",
    "        })\n",
    "\n",
    "print(len(pathgan_blocks))\n",
    "\n",
    "pathgan_blocks_as_tuple = (\n",
    "            (\"dense_block_1\", DenseBlock),\n",
    "            (\"dense_block_2\", DenseBlock),\n",
    "            (\"res_block_1\", ResidualBlock),\n",
    "            (\"upscale_block_1\", ConvolutionalBlock),\n",
    "            (\"res_block_2\", ResidualBlock),\n",
    "            (\"upscale_block_2\", ConvolutionalBlock),\n",
    "            (\"res_block_2\", ResidualBlock),\n",
    "            (\"attention_block\", AttentionBlock),\n",
    "            (\"upscale_block_3\", ConvolutionalBlock),\n",
    "            (\"res_block_3\", ResidualBlock),\n",
    "            (\"upscale_layer_4\", ConvolutionalBlock),\n",
    "            (\"res_block_4\", ResidualBlock),\n",
    "            (\"upscale_block_4\", ConvolutionalBlock),\n",
    "            (\"sigmoid_block\", ConvolutionalBlock),\n",
    "        )\n",
    "\n",
    "print(len(pathgan_blocks_as_tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "len(pathgan_blocks_as_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Parameter containing:\ntensor([[[[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]], requires_grad=True)\nParameter containing:\ntensor([[[[1., 1., 1.],\n          [1., 1., 1.],\n          [1., 1., 1.]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.spectral_norm import SpectralNorm, spectral_norm\n",
    "import copy\n",
    "\n",
    "in_channels, out_channels, kernel_size = 1, 1, 3\n",
    "test_module = nn.Conv2d(in_channels, out_channels, kernel_size)\n",
    "nn.init.constant_(test_module.weight, 1)\n",
    "\n",
    "test_module2 = copy.deepcopy(test_module)\n",
    "\n",
    "spectral_test1 = spectral_norm(test_module)\n",
    "spectral_test2 = spectral_norm(test_module2)\n",
    "\n",
    "print(spectral_test1.weight)\n",
    "print(spectral_test2.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "def fused_scale(weight: Tensor) -> Tensor:\n",
    "        padded = F.pad(weight, [1, 1, 1, 1])\n",
    "        filter = padded[:, :, 1:, 1:] + padded[:, :, 1:, :-1] + padded[:, :, :-1, 1:] + padded[:, :, :-1, :-1]\n",
    "        return filter\n",
    "\n",
    "def _l2normalize(v, eps=1e-12):\n",
    "    return v / (torch.norm(v) + eps)\n",
    "\n",
    "def max_singular_value(W, u=None, Ip=1):\n",
    "    \"\"\"\n",
    "    power iteration for weight parameter\n",
    "    \"\"\"\n",
    "    #xp = W.data\n",
    "    if Ip < 1:\n",
    "        raise ValueError(\"Power iteration should be a positive integer\")\n",
    "    if u is None:\n",
    "        u = torch.FloatTensor(1, W.size(0)).normal_(0, 1).cpu()\n",
    "    _u = u\n",
    "    for _ in range(Ip):\n",
    "        _v = _l2normalize(torch.matmul(_u, W.data), eps=1e-12)\n",
    "        _u = _l2normalize(torch.matmul(_v, torch.transpose(W.data, 0, 1)), eps=1e-12)\n",
    "    sigma = torch.sum(F.linear(_u, torch.transpose(W.data, 0, 1)) * _v) # type: ignore\n",
    "    return sigma, _u\n",
    "\n",
    "def W_(weights, u):\n",
    "    w_mat = weights.view(weights.size(0), -1)\n",
    "    sigma, _u = max_singular_value(w_mat, u)\n",
    "    u.copy_(_u) # type: ignore\n",
    "    return weights / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "tensor = torch.rand(1,1,3,3)\n",
    "u = torch.Tensor(1, 1).normal_()\n",
    "\n",
    "fused_scale_then_spectral = W_(fused_scale(tensor), u)\n",
    "spectral_then_fused_scale = fused_scale(W_(tensor,u))\n",
    "spectral_then_fused_then_spectral = W_(fused_scale(W_(tensor,u)), u)\n",
    "\n",
    "print(torch.allclose(spectral_then_fused_then_spectral, fused_scale_then_spectral))"
   ]
  }
 ]
}